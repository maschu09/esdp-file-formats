{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e22fde4-da97-4c33-a346-04d3406d5956",
   "metadata": {},
   "source": [
    "### Handling Data Produced by the AtmoRep Model\n",
    "This notebook provides a convenient way to work with the output data generated by the AtmoRep model, which is in the zarr format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace602d3-1dec-483d-9ae3-a8918eaa883a",
   "metadata": {},
   "source": [
    "#### The AtmoRep model\n",
    "The AtmoRep model operates on a regional patch-based approach, where the global atmospheric field is divided into smaller, overlapping regional patches (also, called tokens). This approach allows the model to capture local-scale features and dynamics more effectively, while still maintaining a global perspective. The model is implemented as a transformer neural network with 3.5 billion parameters and trained from the ERA5 reanalysis. It can be used for nowcasting (short term forecasting), temporal interpolation, model correction or counterfactuals. This notebook will demonstrate the case of nowcasting. \n",
    "\n",
    "#### ERA5 data\n",
    "ERA5 is the fifth generation global atmospheric reanalysis produced by the European Centre for Medium Range Weather Forecasting (ECMWF). It provides the most consistent and coherent global estimate for the state of the atmosphere, land surface, and ocean waves that is currently available. For AtmoRep, the ERA5 reanalysis dataset is used with an hourly temporal resolution and ERA5’s default equi-angular grid with 721 × 1440 grid points in space. In the vertical dimension, the employed model levels are: 96, 105, 114, 123 and 137, corresponding approximately to pressure levels 546, 693, 850, 947, and 1012 hPa. The available variables are: zonal and meridional wind components, vorticity, divergence, vertical velocity, temperature, specific humidity, and total precipitation.\n",
    "\n",
    "#### Handling AtmoRep data\n",
    "This code contains a Python class, HandleAtmoRepData, which encapsulates the functionality to read, process, and analyze the data produced by the AtmoRep model. The most important methods are: \n",
    "- `get_hierarchical_sorted_files`: goes through the zarr files in the specified directory and outputs a list of the sorted filenames. The files are selected based on the model ID (models with various architecture details are organized using different IDs) and based on the number of epochs used for model training. One can choose only one zarr file that corresponds to a certain epoch or select files from several epochs.\n",
    "- `get_config`: loads the configuration file for a given model ID.\n",
    "- `read_one_file`: reads data from a single output zarr file. It iterates over all the patches and outputs a list of xarrays where each xarray is extracted from one patch.\n",
    "- `read_file`: Merges the output of all the zarr files. It combines the individual data arrays which represent data from different spatial locations or time steps into a global array. The method `get_global_field` makes possible the latter.\n",
    "\n",
    "The `read_file` method encapsulates all the most important steps for retrieving the global data field. It is important that the `get_global_field` method is included in the `read_file` method, since it enables the stitching of the regional data patches into a cohesive global data field.\n",
    "\n",
    "The use of the zarr file format enables efficient I/O operations thanks to chunking.\n",
    "\n",
    "#### Usage\n",
    "This code scans the data in zarr format after you download them in your machine. One can experiment on extracting the ground truth and prediction for various variables such as: specific humidity, temperature, total precipitation, velocity and temperature. The model prediction on the demonstrated case consists on five future time stamps. Through the last routine, five global plots can be generated for each variable. Each plot consists on one time stamp. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391c74a7-eff3-49e1-92ed-c00bef4a9de1",
   "metadata": {},
   "source": [
    "#### Source Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4888bdef-ffd7-4ca9-bfc4-66907de6a850",
   "metadata": {},
   "source": [
    "##### Necessary python packages to be imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a2933a-3287-4c55-82c6-432e28f40ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import code\n",
    "import zarr\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "import cartopy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['axes.linewidth'] = 0.1\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcccd8b-6a91-4789-b837-934e37da565b",
   "metadata": {},
   "source": [
    "##### HandleAtmoRepData class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b176fb65-8e17-4f82-a945-41635e63187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandleAtmoRepData(object):\n",
    "    \"\"\"\n",
    "    Handle output data of AtmoRep model.\n",
    "    \"\"\"\n",
    "    known_data_types = [\"source\", \"pred\", \"target\", \"ens\"]\n",
    "    \n",
    "    def __init__(self, model_id: str, results_dir: str):\n",
    "        \"\"\"\n",
    "        :param model_id: ID of Atmorep-run to process\n",
    "        :param results_dir: top-directory where results are stored\n",
    "        \"\"\"\n",
    "        self.model_id = model_id if model_id.startswith(\"id\") else f\"id{model_id}\"\n",
    "        self.results_dir = results_dir\n",
    "        self.config_file, self.config = self._get_config()\n",
    "\n",
    "    def _get_config(self) -> (str, dict):\n",
    "        \"\"\"\n",
    "        Get configuration dictionary of trained AtmoRep-model.\n",
    "        \"\"\"\n",
    "        config_jsf = self.results_dir.joinpath(f\"model_{self.model_id}.json\")  \n",
    "        with open(config_jsf) as json_file:\n",
    "            config = json.load(json_file)         \n",
    "        return config_jsf, config\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_number(file_name, split_arg):\n",
    "        \"\"\" \n",
    "        Extract the number from the file name using the provided split argument.\n",
    "        \"\"\"\n",
    "        return int(str(file_name).split(split_arg)[1].split('_')[0])  \n",
    "\n",
    "    def get_hierarchical_sorted_files(self, data_type: str, epoch: int = -1):\n",
    "        \"\"\"\n",
    "        Get sorted list of file names based on the specified data type and epoch.\n",
    "        :param data_type: Type of data which should be retrieved (either 'source', 'target', 'ens' or 'pred')\n",
    "        :param epoch: number of epoch for which data should be retrieved. Parse -1 for getting all epochs.\n",
    "        \"\"\"\n",
    "        epoch_str = f\"epoch*\" if epoch == -1 else f\"epoch{epoch:05d}\"\n",
    "        fpatt = f\"results_{self.model_id}_{epoch_str}_{data_type}.zarr\"\n",
    "        file_list = list(Path(self.results_dir).glob(f\"**/{fpatt}\"))\n",
    "        if not file_list:\n",
    "            raise FileNotFoundError(f\"No files found matching '{fpatt}' in '{self.results_dir}'.\")            \n",
    "        return sorted(file_list, key=lambda x: self.get_number(x, \"_epoch\"))\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_global_field(da_list):\n",
    "        \"\"\"\n",
    "        Combines the individual data arrays which may represent data from different spatial locations or time steps,\n",
    "        into a single global data array.\n",
    "        \"\"\"\n",
    "        # get unique time stamps\n",
    "        times_unique = list(set([time for da in da_list for time in da[\"datetime\"].values]))\n",
    "        dx, dy = np.abs(da_list[0][\"lon\"][1] - da_list[0][\"lon\"][0]), \\\n",
    "                 np.abs(da_list[0][\"lat\"][1] - da_list[0][\"lat\"][0])\n",
    "        \n",
    "        # initialize empty global data array\n",
    "        dims = da_list[0].dims\n",
    "        data_coords = {k: v for k, v in da_list[0].coords.items() if k not in [\"lat\", \"lon\"]}\n",
    "        data_coords[\"lat\"] = np.linspace(-90., 90., num=int(180/dy) + 1, endpoint=True)\n",
    "        data_coords[\"lon\"] = np.linspace(0, 360, num=int(360/dx), endpoint=False)  \n",
    "        data_coords[\"datetime\"] = times_unique\n",
    "    \n",
    "        da_global = xr.DataArray(np.empty(tuple(len(d) for d in data_coords.values())), \n",
    "                                 coords=data_coords, dims=dims)\n",
    "        # fill global data array \n",
    "        for da in da_list:\n",
    "            da_global.loc[{\"datetime\": da[\"datetime\"], \"lat\": da[\"lat\"], \"lon\": da[\"lon\"]}] = da\n",
    "    \n",
    "        if np.any(da_global.isnull()): \n",
    "            raise ValueError(f\"Could not get global data field.\")             \n",
    "        return da_global  \n",
    "\n",
    "    def read_one_file(self, fname: str, varname: str, data_type: str):\n",
    "        \"\"\"\n",
    "        Reads data from a single output file of AtmoRep. It creates a zarr.ZipStore object from the fname and a zarr.group \n",
    "        object from the store. Iterates over the patches in the Zarr group and constructs a list of \n",
    "        xarray.DataArray objects, one for each patch. Each xarray.DataArray object is created by extracting the data and \n",
    "        coordinates from the Zarr group.\n",
    "        Finally, the function returns the list of xarray.DataArray objects.\n",
    "        :param fname: Name of zarr-file that should be read\n",
    "        :param varname: name of variable in zarr-file to be accessed\n",
    "        :param data_type: Type of data which should be retrieved (either 'source', 'target', 'ens' or 'pred')\n",
    "        :return: list of DataArrays where each element provides one sample\n",
    "        \"\"\"    \n",
    "        store = zarr.ZipStore(fname, mode='r')\n",
    "        grouped_store = zarr.group(store)\n",
    "            \n",
    "        dims = [\"ml\", \"datetime\", \"lat\", \"lon\"]\n",
    "        if data_type == \"ens\":\n",
    "            nens = self.config[\"net_tail_num_nets\"]\n",
    "            coords = {\"ensemble\": range(nens)}\n",
    "        else:\n",
    "            coords = {}\n",
    "            \n",
    "        da = []\n",
    "        for ip, patch in tqdm(enumerate(grouped_store[os.path.join(varname)])):\n",
    "            coords.update({dim: grouped_store[os.path.join(varname, patch, dim)] for dim in dims})\n",
    "            da_p = xr.DataArray(grouped_store[os.path.join(varname, patch, \"data\")], coords=coords,                \n",
    "                                dims = [\"ensemble\"] + dims if data_type == \"ens\" else dims, name=f\"{varname}_{patch.replace('=', '')}\")\n",
    "            da.append(da_p)       \n",
    "        return da\n",
    "        \n",
    "    def read_data(self, varname: str, data_type, epoch: int = -1, **kwargs):\n",
    "        \"\"\"\n",
    "        Coordinates the reading of data from the output files and returns the combined data as a list of xarray.DataArray objects.\n",
    "        Further analysis can be done by using these data. \n",
    "        :param varname: name of variable for which token info is requested\n",
    "        :param data_type: Type of data which should be retrieved (either 'source', 'target', 'ens' or 'pred')\n",
    "        :param epoch: training epoch of requested token information file\n",
    "        \"\"\"                  \n",
    "        assert data_type in self.known_data_types, f\"Data type '{data_type}' is unknown. Choose one of the following: '{', '.join(self.known_data_types)}'\"\n",
    "        \n",
    "        file_list = self.get_hierarchical_sorted_files(data_type, epoch)\n",
    "        \n",
    "        if self.config[\"BERT_strategy\"] == \"forecast\":\n",
    "            args = {\"varname\": varname, \"data_type\": data_type}\n",
    "        \n",
    "        print(f\"Start reading {len(file_list)} files...\")\n",
    "        da = []\n",
    "        for i, f in enumerate(file_list):\n",
    "            da += self.read_one_file(f, **args)\n",
    "            \n",
    "        # return global data if global forecasting evaluation mode was chosen \n",
    "        if self.config[\"BERT_strategy\"] == \"forecast\" and self.config.get(\"token_overlap\", False):\n",
    "            da = self.get_global_field(da)   \n",
    "        return da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8f3259-151f-4810-abaa-ac40f2a42d8a",
   "metadata": {},
   "source": [
    "##### A routine that plots a global map for the selected variable\n",
    "This routine loads the data from the zarr file. One patch contains data from several time stamps and each time stamp creates one plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48066980-d4c8-4769-a634-ebafd9c801e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_global_map(input_dir, field, model_id):\n",
    "    # create empty canvas where local patches can be filled in\n",
    "    store = zarr.ZipStore(f'{input_dir}/results_id{model_id}_epoch00000_pred.zarr', mode='r')\n",
    "    ds = zarr.group(store=store)\n",
    "    i = 0\n",
    "    ds_o = xr.Dataset( coords={ 'ml' : ds[ f'{field}/sample={i:05d}/ml' ][:],\n",
    "                                'datetime': ds[ f'{field}/sample={i:05d}/datetime' ][:], \n",
    "                                'lat' : np.linspace( -90., 90., num=180*4+1, endpoint=True), \n",
    "                                'lon' : np.linspace( 0., 360., num=360*4, endpoint=False) } )\n",
    "    print(\"The plotted time stamps: \", ds[ f'{field}/sample={i:05d}/datetime' ][:])\n",
    "    nlevels = ds[ f'{field}/sample={i:05d}/ml' ].shape[0]\n",
    "    ds_o['vo'] = (['ml', 'datetime', 'lat', 'lon'], np.zeros( ( nlevels, 6, 721, 1440)))\n",
    "    \n",
    "    # fill in local patches\n",
    "    all_lats=[]\n",
    "    all_lons=[]\n",
    "    for i_str in ds[ f'{field}']:\n",
    "      if np.any(ds[ f'{field}/{i_str}/datetime' ][:]  != ds_o['vo'].datetime):\n",
    "        break\n",
    "      ds_o['vo'].loc[ dict( datetime=ds[ f'{field}/{i_str}/datetime' ][:],\n",
    "            lat=ds[ f'{field}/{i_str}/lat' ][:],\n",
    "            lon=ds[ f'{field}/{i_str}/lon' ][:]) ] = ds[ f'{field}/{i_str}/data'][:] #[0, :]\n",
    "    \n",
    "    # plot and save the time steps that form a token\n",
    "    cmap = 'RdBu_r'\n",
    "    vmin, vmax = ds_o['vo'].values[0].min(), ds_o['vo'].values[0].max()\n",
    "    print(ds_o['datetime'].shape)\n",
    "    for k in range( 6) :\n",
    "      fig = plt.figure( figsize=(10,5), dpi=300)\n",
    "      ax = plt.axes( projection=cartopy.crs.Robinson( central_longitude=0.))\n",
    "      ax.add_feature( cartopy.feature.COASTLINE, linewidth=0.5, edgecolor='k', alpha=0.5)\n",
    "      ax.set_global()\n",
    "      date = ds_o['datetime'].values[k].astype('datetime64[m]')\n",
    "      ax.set_title(f'{field} : {date}')\n",
    "      ds_o['vo'].isel(ml=0, datetime = k).plot.imshow(cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "      im = ax.imshow( np.flip(ds_o['vo'].values[0,k], 0), cmap=cmap, vmin=vmin, vmax=vmax,\n",
    "                      transform=cartopy.crs.PlateCarree( central_longitude=180.))\n",
    "      axins = inset_axes( ax, width=\"80%\", height=\"5%\", loc='lower center', borderpad=-2 )\n",
    "      fig.colorbar( im, cax=axins, orientation=\"horizontal\")\n",
    "      plt.show(f'example_{k:03d}.png')\n",
    "      #plt.savefig(f'example_{k:03d}.png')\n",
    "      plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9da7391-3113-42fd-b9a1-dd5794246ed1",
   "metadata": {},
   "source": [
    "##### Instantiating an object of the HandleAtmoRepData class and calling methods on that object\n",
    "Extracting target and prediction data by calling the `read_data` method. This method wraps the methods mentioned in the **Handling AtmoRep Data** section. We specify the model id, the variable of interest and the path of the folder where the zarr file is located. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ea5af-678f-4692-947a-0b7accc4e00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'c96xrbip'\n",
    "field = 'temperature'\n",
    "input_dir = Path(\"/home/enxhi/Documents/data/atmorep_zarr2/\")\n",
    "\n",
    "ar_data    = HandleAtmoRepData(model_id, input_dir)\n",
    "da_target  = ar_data.read_data(field, \"target\")\n",
    "da_pred    = ar_data.read_data(field, \"pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd208716-5a99-4178-9352-700245cce57b",
   "metadata": {},
   "source": [
    "##### Calling the routine which plots the global map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1439c3-abb8-42d2-90c2-8de0e5fff48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_global_map(input_dir, field, model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
